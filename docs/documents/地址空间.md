# 地址空间和内存分配

## 内存分配

Rust 的标准库中提供了很多开箱即用的堆数据结构，利用它们能够大大提升我们的开发效率。

如果要在操作系统内核中支持 Rust 动态内存分配，则需按下图介绍实现一系列功能：初始化堆地址、alloc/dealloc内存块的函数接口、动态内存分配器等等。

![堆内存分配 ](C:\Users\wakaka\Pictures\Camera Roll\堆内存分配 .jpg)

为此，我们可以在 Tcore 中增加连续内存分配的功能，具体实现主要集中在 `Tcore/codes/os/src/mm/heap_allocator.rs` 中,完成这一步后，我们便可在 Tcore 中用到 Rust 的堆数据结构了，如 `Vec` 、 `Box` 等，这样内核编程就更加方便灵活了。



## 内核地址空间



![内核地址空间](https://img-1307557302.cos.ap-shanghai.myqcloud.com/img/%E5%86%85%E6%A0%B8%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4.png)



从操作系统启动开始，内核地址空间就需要载入启动所需要的数据，并且载入0号初始进程 initproc 以及用户命令行进程usershell 。在运行程序的过程中，内核地址空间划分成多个应用程序各自对用的内核栈空间，并且通过 Guard Page 机制来分隔开——两个内核栈之间会预留一个 **保护页面** (Guard Page) ，它是内核地址空间中的空洞，多级页表中并不存在与它相关的映射。它的意义在于当内核栈空间不足（如调用层数过多或死递归）的时候，代码会尝试访问空洞区域内的虚拟地址，然而它无法在多级页表中找到映射，便会触发异常，此时控制权会交给内核 trap handler 函数进行异常处理。。

可以看到，跳板放在最高的一个虚拟页面中。接下来则是从高到低放置每个应用的内核栈，内核栈的大小由 `config` 子模块的 `KERNEL_STACK_SIZE` 给出。它们的映射方式为 `MapPermission` 中的 rw 两个标志位，意味着这个逻辑段仅允许 CPU 处于内核态访问，且只能读或写。

## 用户地址空间

![内核地址空间](https://img-1307557302.cos.ap-shanghai.myqcloud.com/img/%E5%86%85%E6%A0%B8%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4-16514876804562.png)

右图为用户地址空间分布。用户地址主要通过 MemorySet  的数据结构来管理，并且通过页表与物理地址空间映射，每次新建一个用户应用程序对应的进程时，都会新建一个专属的 MemorySet ，并且通过from_elf 接口从二进制文件中读取数据。MemorySet 下面包含用户地址中的每一段空间（data,text,heap,stack,…），并且分别用一个单独的数据结构表示，在新建时都会通过map 函数在页表中建立映射。在进行fork 或者clone 的时候，MemorySet 也提供了专门的接口来实现通过这两种系统调用而生成新的进程的MemorySet 的方法。

跳板在用户地址空间的最顶部，将虚地址和实地址联系起来（跳板的虚地址和实地址对于每一个进程都是相同的），用于在不同的用户进程空间中进行切换，并且可以在trap陷入的时候，将TrapContext保存在此处。

用户地址的栈空间用于函数之间相互调用时，传递参数，在用户栈底下的是该应用程序的预分配的静态内存空间。

sys_brk 也是通过sbrk 的系统调用实现的。sys_sbrk 是用户堆空间分配动态内存的接口，可以对某个进程的动态内存进行增长和收缩，通过grow_proc()函数实现。堆空间也是从属于所在进程的MemorySet 之中的，在最开始空间大小为0，并且堆指针随着空间的增长与收缩而上下移动。通过grow_proc 函数，用户内存在堆空间中增长响应的大小，并且把这一整段内存地址传给buddy_allocator，于是通过BuddyAllocator 的new()可以在应用程序中获取随即大小的动态内存空间。



## mmap和munmap设计

Linux通过内存映像机制来提供用户程序对内存直接访问的能力。内存映像的意思是把内核中特定部分的内存空间映射到用户级程序的内存空间去。也就是说，用户空间和内核空间共享一块相同的内存。这样做的直观效果显而易见：内核在这块地址内存储变更的任何数据，用户可以立即发现和使用，根本无须数据拷贝。举个例子理解一下，使用 mmap 方式获取磁盘上的文件信息，只需要将磁盘上的数据拷贝至那块共享内存中去，用户进程可以直接获取到信息，而相对于传统的write/read　IO系统调用, 必须先把数据从磁盘拷贝至到内存缓冲区，然后再把数据拷贝至用户进程中。两者相比，mmap会少一次拷贝数据，这样带来的性能提升是巨大的。

在Tcore中是通过 sys_mmap 和 sys_munmap 的系统调用，可以在文件磁盘块和内存空间之间建立映射关系，此时就可以将文件块当成内存空间来进行读写的操作。需要某个磁盘块在第一次被进程调用时，在内存地址中开辟对应的空间，并且通过mmap 进行
映射，将其映射到一段虚拟内存地址中，并且建立页表。在最开始访问这段内存时，触发页表的invalid异常，此时异常处理中断会通过文件操作读取磁盘块中的数据并且拷贝到内存中；此后每次就可以直接对这段内存数据进行操作了。并且有新的其它进程要访问此文件时，再次调用 sys_mmap 可以直接共享这段内存空间，并根据它的权限标记来判断是否可以共享读/写这段内存。

当一个进程访问完这个文件块需要释放时，调用 sys_munmap , 如果所有进程绑定的都对这段空间进行了 munmap , 就会通过系统调用将这段映射解除。



![mmap文件缓冲区](https://img-1307557302.cos.ap-shanghai.myqcloud.com/img/mmap%E6%96%87%E4%BB%B6%E7%BC%93%E5%86%B2%E5%8C%BA.png)

mmap 的空间也是在进程建立时，就在 MemorySet 中建立了一个相应的数据结构进行分配的，但是在 MemeorySet 中并不能很灵活地管理mmap 的虚拟地址空间的分配，因此加入了 MmapArea  和 MmapSpace 的数据结构来专门管理磁盘文件块和对应的物理缓冲区与虚拟内存之间的映射关系以及页表的建立，这两个数据结构本身并不分配内存空间，但是调用页表的接口来建立虚实地址的映射，并且本身记录了mmap 相关的全部信息，以便动态地建立和调整 mmap 空间。每当建立新的 mmap 时，先向 Mmap 传递将要映射的文件大小，并且获得可以建立页表映射的那段虚地址，然后在 TaskControlBlock 中完成地址的映射。随后进入为这段 mmap 而新建的 MmapSpace，通过 Fat32 的文件接口将这段文件内容读出并且写入到对应的物理缓冲区地址中，完成 mmap 系统调用。



## Lazy 策略——延迟分配

Lazy思想在许多地方都能看见，在一些资源不够用时，可发挥重要作用。如Linux总是以Lazy的方式给应用程序分配内存，包括堆、栈(函数调用越深，用的栈越多，最终发生page fault才得到栈)、代码段、数据段。这能使操作系统更充分的使用较为局限的资源，我们的 K210 开发板目前只有 8MiB 的内存，因此从空间占用角度来说，Lazy策略是非常优秀的。

在一开始的Tcore中，我们采用预先分配方式来为每一个进程预留一段固定大小的堆/栈空间，但实际上这段空间在大多数程序的运行过程中并未别完全使用，或是仅使用了一部分。因此，若采用Lazy策略，随着堆的增长（brk）和栈的增长（push）来动态地增长，当需要使用到某个虚拟页的时候再给它分配物理页帧，可以节省足够的空间，以免在高强度创建进程的时候，导致可分配的物理内存空间不足进而panic。

但对于栈空间Stack来说，因为执行流的切换，每个程序至少会用到一定大小的内存，这造成了每个程序栈空间有一部分是保底使用。若完全在Lazy的策略下，每次都要发生page fault，进入trap去给他们分配，实际上拖慢了系统的执行速度，我们参考Linux的方法，恰好给每个进程提前分配少量固定的堆栈空间，对于堆栈使用强度小的大部分程序，这些提前分配的空间恰好足够他们使用，不触发 lazy alloc，对于少部分强度大的程序，后面分配的空间不一定全部都被使用，因此当用到时再触发trap来分配。

